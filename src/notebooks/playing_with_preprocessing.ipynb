{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Simple TextRank implementation\n",
    "\n",
    "In this notebook I load, preprocess xsum dataset, and create very simple TextRank based on PageRank."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 21:54:41.275255: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "[nltk_data] Downloading package punkt to /home/pasha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/pasha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import trange\n",
    "import time\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# stop_words = stopwords.words('english')1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# import scipy\n",
    "# import networkx as nx\n",
    "# scipy = importlib.reload(scipy)\n",
    "# nx = importlib.reload(nx)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prepare embedings for later"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# create embedings for each word\n",
    "word_embeddings = {}\n",
    "f = open('../..//data/glove/glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embedings: 100\n",
      "Embeding for hello: [ 0.26688    0.39632    0.6169    -0.77451   -0.1039     0.26697\n",
      "  0.2788     0.30992    0.0054685 -0.085256   0.73602   -0.098432\n",
      "  0.5479    -0.030305   0.33479    0.14094   -0.0070003  0.32569\n",
      "  0.22902    0.46557   -0.19531    0.37491   -0.7139    -0.51775\n",
      "  0.77039    1.0881    -0.66011   -0.16234    0.9119     0.21046\n",
      "  0.047494   1.0019     1.1133     0.70094   -0.08696    0.47571\n",
      "  0.1636    -0.44469    0.4469    -0.93817    0.013101   0.085964\n",
      " -0.67456    0.49662   -0.037827  -0.11038   -0.28612    0.074606\n",
      " -0.31527   -0.093774  -0.57069    0.66865    0.45307   -0.34154\n",
      " -0.7166    -0.75273    0.075212   0.57903   -0.1191    -0.11379\n",
      " -0.10026    0.71341   -1.1574    -0.74026    0.40452    0.18023\n",
      "  0.21449    0.37638    0.11239   -0.53639   -0.025092   0.31886\n",
      " -0.25013   -0.63283   -0.011843   1.377      0.86013    0.20476\n",
      " -0.36815   -0.68874    0.53512   -0.46556    0.27389    0.4118\n",
      " -0.854     -0.046288   0.11304   -0.27326    0.15636   -0.20334\n",
      "  0.53586    0.59784    0.60469    0.13735    0.42232   -0.61279\n",
      " -0.38486    0.35842   -0.48464    0.30728  ]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Size of embedings: {len(word_embeddings['hello'])}\")\n",
    "print(f\"Embeding for hello: {word_embeddings['hello']}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset xsum (/home/pasha/.cache/huggingface/datasets/xsum/default/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9f49e5714da3407c8e6093f0c56dfdb0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load dataset\n",
    "ds = load_dataset(\"xsum\", \"default\", keep_in_memory=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['document', 'summary', 'id'],\n",
      "        num_rows: 204045\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['document', 'summary', 'id'],\n",
      "        num_rows: 11332\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['document', 'summary', 'id'],\n",
      "        num_rows: 11334\n",
      "    })\n",
      "})\n",
      "A former Lincolnshire Police officer carried out a series of sex attacks on boys, a jury at Lincoln Crown Court was told.\n"
     ]
    }
   ],
   "source": [
    "# checkout dataset\n",
    "# 1. note - size of summary is always 1 sentence\n",
    "print(ds)\n",
    "print(ds['train']['summary'][3])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part: train\n",
      "Part: validation\n",
      "Part: test\n"
     ]
    }
   ],
   "source": [
    "# Let's try to merge train and val and test\n",
    "def get_docs_sums():\n",
    "    documents = []\n",
    "    summaries = []\n",
    "    for part in ['train', 'validation', 'test']:\n",
    "        print(f\"Part: {part}\")\n",
    "        documents.extend(ds[part]['document'])\n",
    "        summaries.extend(ds[part]['summary'])\n",
    "    return documents, summaries\n",
    "\n",
    "documents, summaries = get_docs_sums()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# for i in trange(100):\n",
    "#     if \"@\" in documents[i]:\n",
    "#         print(i)\n",
    "#         break\n",
    "#\n",
    "# documents[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–Ž         | 8304/226711 [00:02<01:16, 2848.91it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [9]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# now let's divide documents into sentences\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# splitted_docs = []\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m di \u001B[38;5;129;01min\u001B[39;00m trange(\u001B[38;5;28mlen\u001B[39m(documents)):\n\u001B[0;32m----> 4\u001B[0m     documents[di] \u001B[38;5;241m=\u001B[39m \u001B[43mnltk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenize\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msent_tokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdocuments\u001B[49m\u001B[43m[\u001B[49m\u001B[43mdi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/ml_project_v2/lib/python3.9/site-packages/nltk/tokenize/__init__.py:107\u001B[0m, in \u001B[0;36msent_tokenize\u001B[0;34m(text, language)\u001B[0m\n\u001B[1;32m     97\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;124;03mReturn a sentence-tokenized copy of *text*,\u001B[39;00m\n\u001B[1;32m     99\u001B[0m \u001B[38;5;124;03musing NLTK's recommended sentence tokenizer\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    104\u001B[0m \u001B[38;5;124;03m:param language: the model name in the Punkt corpus\u001B[39;00m\n\u001B[1;32m    105\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    106\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m load(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtokenizers/punkt/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlanguage\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.pickle\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 107\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/ml_project_v2/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1276\u001B[0m, in \u001B[0;36mPunktSentenceTokenizer.tokenize\u001B[0;34m(self, text, realign_boundaries)\u001B[0m\n\u001B[1;32m   1272\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtokenize\u001B[39m(\u001B[38;5;28mself\u001B[39m, text, realign_boundaries\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[1;32m   1273\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1274\u001B[0m \u001B[38;5;124;03m    Given a text, returns a list of the sentences in that text.\u001B[39;00m\n\u001B[1;32m   1275\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1276\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msentences_from_text\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrealign_boundaries\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/anaconda3/envs/ml_project_v2/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1332\u001B[0m, in \u001B[0;36mPunktSentenceTokenizer.sentences_from_text\u001B[0;34m(self, text, realign_boundaries)\u001B[0m\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msentences_from_text\u001B[39m(\u001B[38;5;28mself\u001B[39m, text, realign_boundaries\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[1;32m   1326\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1327\u001B[0m \u001B[38;5;124;03m    Given a text, generates the sentences in that text by only\u001B[39;00m\n\u001B[1;32m   1328\u001B[0m \u001B[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001B[39;00m\n\u001B[1;32m   1329\u001B[0m \u001B[38;5;124;03m    True, includes in the sentence closing punctuation that\u001B[39;00m\n\u001B[1;32m   1330\u001B[0m \u001B[38;5;124;03m    follows the period.\u001B[39;00m\n\u001B[1;32m   1331\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1332\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [text[s:e] \u001B[38;5;28;01mfor\u001B[39;00m s, e \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001B[0;32m~/anaconda3/envs/ml_project_v2/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1332\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msentences_from_text\u001B[39m(\u001B[38;5;28mself\u001B[39m, text, realign_boundaries\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[1;32m   1326\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1327\u001B[0m \u001B[38;5;124;03m    Given a text, generates the sentences in that text by only\u001B[39;00m\n\u001B[1;32m   1328\u001B[0m \u001B[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001B[39;00m\n\u001B[1;32m   1329\u001B[0m \u001B[38;5;124;03m    True, includes in the sentence closing punctuation that\u001B[39;00m\n\u001B[1;32m   1330\u001B[0m \u001B[38;5;124;03m    follows the period.\u001B[39;00m\n\u001B[1;32m   1331\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1332\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [text[s:e] \u001B[38;5;28;01mfor\u001B[39;00m s, e \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001B[0;32m~/anaconda3/envs/ml_project_v2/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1322\u001B[0m, in \u001B[0;36mPunktSentenceTokenizer.span_tokenize\u001B[0;34m(self, text, realign_boundaries)\u001B[0m\n\u001B[1;32m   1320\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m realign_boundaries:\n\u001B[1;32m   1321\u001B[0m     slices \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_realign_boundaries(text, slices)\n\u001B[0;32m-> 1322\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m sentence \u001B[38;5;129;01min\u001B[39;00m slices:\n\u001B[1;32m   1323\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m (sentence\u001B[38;5;241m.\u001B[39mstart, sentence\u001B[38;5;241m.\u001B[39mstop)\n",
      "File \u001B[0;32m~/anaconda3/envs/ml_project_v2/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1421\u001B[0m, in \u001B[0;36mPunktSentenceTokenizer._realign_boundaries\u001B[0;34m(self, text, slices)\u001B[0m\n\u001B[1;32m   1408\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1409\u001B[0m \u001B[38;5;124;03mAttempts to realign punctuation that falls after the period but\u001B[39;00m\n\u001B[1;32m   1410\u001B[0m \u001B[38;5;124;03mshould otherwise be included in the same sentence.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1418\u001B[0m \u001B[38;5;124;03m    [\"(Sent1.)\", \"Sent2.\"].\u001B[39;00m\n\u001B[1;32m   1419\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1420\u001B[0m realign \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m-> 1421\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m sentence1, sentence2 \u001B[38;5;129;01min\u001B[39;00m _pair_iter(slices):\n\u001B[1;32m   1422\u001B[0m     sentence1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mslice\u001B[39m(sentence1\u001B[38;5;241m.\u001B[39mstart \u001B[38;5;241m+\u001B[39m realign, sentence1\u001B[38;5;241m.\u001B[39mstop)\n\u001B[1;32m   1423\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m sentence2:\n",
      "File \u001B[0;32m~/anaconda3/envs/ml_project_v2/lib/python3.9/site-packages/nltk/tokenize/punkt.py:321\u001B[0m, in \u001B[0;36m_pair_iter\u001B[0;34m(iterator)\u001B[0m\n\u001B[1;32m    319\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[1;32m    320\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m--> 321\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m el \u001B[38;5;129;01min\u001B[39;00m iterator:\n\u001B[1;32m    322\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m (prev, el)\n\u001B[1;32m    323\u001B[0m     prev \u001B[38;5;241m=\u001B[39m el\n",
      "File \u001B[0;32m~/anaconda3/envs/ml_project_v2/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1396\u001B[0m, in \u001B[0;36mPunktSentenceTokenizer._slices_from_text\u001B[0;34m(self, text)\u001B[0m\n\u001B[1;32m   1394\u001B[0m last_break \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m   1395\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m match, context \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_match_potential_end_contexts(text):\n\u001B[0;32m-> 1396\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtext_contains_sentbreak\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m   1397\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28mslice\u001B[39m(last_break, match\u001B[38;5;241m.\u001B[39mend())\n\u001B[1;32m   1398\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m match\u001B[38;5;241m.\u001B[39mgroup(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnext_tok\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m   1399\u001B[0m             \u001B[38;5;66;03m# next sentence starts after whitespace\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/ml_project_v2/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1442\u001B[0m, in \u001B[0;36mPunktSentenceTokenizer.text_contains_sentbreak\u001B[0;34m(self, text)\u001B[0m\n\u001B[1;32m   1438\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1439\u001B[0m \u001B[38;5;124;03mReturns True if the given text includes a sentence break.\u001B[39;00m\n\u001B[1;32m   1440\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1441\u001B[0m found \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m  \u001B[38;5;66;03m# used to ignore last token\u001B[39;00m\n\u001B[0;32m-> 1442\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m tok \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_annotate_tokens(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tokenize_words(text)):\n\u001B[1;32m   1443\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m found:\n\u001B[1;32m   1444\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/ml_project_v2/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1578\u001B[0m, in \u001B[0;36mPunktSentenceTokenizer._annotate_second_pass\u001B[0;34m(self, tokens)\u001B[0m\n\u001B[1;32m   1572\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_annotate_second_pass\u001B[39m(\u001B[38;5;28mself\u001B[39m, tokens):\n\u001B[1;32m   1573\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1574\u001B[0m \u001B[38;5;124;03m    Performs a token-based classification (section 4) over the given\u001B[39;00m\n\u001B[1;32m   1575\u001B[0m \u001B[38;5;124;03m    tokens, making use of the orthographic heuristic (4.1.1), collocation\u001B[39;00m\n\u001B[1;32m   1576\u001B[0m \u001B[38;5;124;03m    heuristic (4.1.2) and frequent sentence starter heuristic (4.1.3).\u001B[39;00m\n\u001B[1;32m   1577\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1578\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m token1, token2 \u001B[38;5;129;01min\u001B[39;00m _pair_iter(tokens):\n\u001B[1;32m   1579\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_second_pass_annotation(token1, token2)\n\u001B[1;32m   1580\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m token1\n",
      "File \u001B[0;32m~/anaconda3/envs/ml_project_v2/lib/python3.9/site-packages/nltk/tokenize/punkt.py:318\u001B[0m, in \u001B[0;36m_pair_iter\u001B[0;34m(iterator)\u001B[0m\n\u001B[1;32m    316\u001B[0m iterator \u001B[38;5;241m=\u001B[39m \u001B[38;5;28miter\u001B[39m(iterator)\n\u001B[1;32m    317\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 318\u001B[0m     prev \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    319\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[1;32m    320\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/ml_project_v2/lib/python3.9/site-packages/nltk/tokenize/punkt.py:598\u001B[0m, in \u001B[0;36mPunktBaseClass._annotate_first_pass\u001B[0;34m(self, tokens)\u001B[0m\n\u001B[1;32m    581\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_annotate_first_pass\u001B[39m(\u001B[38;5;28mself\u001B[39m, tokens):\n\u001B[1;32m    582\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    583\u001B[0m \u001B[38;5;124;03m    Perform the first pass of annotation, which makes decisions\u001B[39;00m\n\u001B[1;32m    584\u001B[0m \u001B[38;5;124;03m    based purely based on the word type of each word:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    596\u001B[0m \u001B[38;5;124;03m      - ellipsis_toks: The indices of all ellipsis marks.\u001B[39;00m\n\u001B[1;32m    597\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 598\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m aug_tok \u001B[38;5;129;01min\u001B[39;00m tokens:\n\u001B[1;32m    599\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_first_pass_annotation(aug_tok)\n\u001B[1;32m    600\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m aug_tok\n",
      "File \u001B[0;32m~/anaconda3/envs/ml_project_v2/lib/python3.9/site-packages/nltk/tokenize/punkt.py:569\u001B[0m, in \u001B[0;36mPunktBaseClass._tokenize_words\u001B[0;34m(self, plaintext)\u001B[0m\n\u001B[1;32m    566\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[1;32m    567\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m--> 569\u001B[0m \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_Token\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtok\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparastart\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparastart\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlinestart\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    570\u001B[0m parastart \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    572\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m tok \u001B[38;5;129;01min\u001B[39;00m line_toks:\n",
      "File \u001B[0;32m~/anaconda3/envs/ml_project_v2/lib/python3.9/site-packages/nltk/tokenize/punkt.py:406\u001B[0m, in \u001B[0;36mPunktToken.__init__\u001B[0;34m(self, tok, **params)\u001B[0m\n\u001B[1;32m    404\u001B[0m     \u001B[38;5;28msetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, prop, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    405\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m params:\n\u001B[0;32m--> 406\u001B[0m     \u001B[38;5;28msetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, k, \u001B[43mparams\u001B[49m\u001B[43m[\u001B[49m\u001B[43mk\u001B[49m\u001B[43m]\u001B[49m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# now let's divide documents into sentences\n",
    "# splitted_docs = []\n",
    "for di in trange(len(documents)):\n",
    "    documents[di] = nltk.tokenize.sent_tokenize(documents[di])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(f\"Number of sentences in document {i}: {len(documents[i])}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Very basic text preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def preprocess_ds(func):\n",
    "   for di in trange(len(documents)):\n",
    "       for si, sentence in enumerate(documents[di]):\n",
    "           documents[di][si] = func(sentence)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 226711/226711 [00:07<00:00, 30702.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# 1. remove non ascii symbols\n",
    "preprocess_ds(lambda s: re.sub(r\"$[^a-zA-Z]\", \" \", s))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 226711/226711 [00:01<00:00, 226060.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# 2. To lowercase\n",
    "preprocess_ds(lambda s: s.lower())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 226711/226711 [03:06<00:00, 1212.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# 3. remove stopwords\n",
    "def remove_stopwords(sen: str):\n",
    "    sen_new = \" \".join([i for i in sen.split() if i not in stop_words])\n",
    "    return sen_new\n",
    "\n",
    "preprocess_ds(remove_stopwords)\n",
    "\n",
    "# has_not = False\n",
    "# for sen in documents[0]:\n",
    "#     has_not = has_not or \"not\" in sen\n",
    "# print(f\"Has not: {has_not}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part: train\n",
      "Part: validation\n",
      "Part: test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 226711/226711 [01:23<00:00, 2719.46it/s]\n"
     ]
    }
   ],
   "source": [
    "orig_documents, _ = get_docs_sums()\n",
    "for di in trange(len(orig_documents)):\n",
    "    orig_documents[di] = nltk.tokenize.sent_tokenize(orig_documents[di])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The full cost of damage in Newton Stewart, one of the areas worst affected, is still being assessed.\n"
     ]
    }
   ],
   "source": [
    "print(orig_documents[0][0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full cost damage newton stewart, one areas worst affected, still assessed.\n"
     ]
    }
   ],
   "source": [
    "print(documents[0][0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 226711/226711 [01:10<00:00, 3215.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# 4. Vectorize with glove\n",
    "def glove_vectorize(sentence):\n",
    "    if len(sentence) != 0:\n",
    "        return sum([word_embeddings.get(w, np.zeros((100,))) for w in sentence.split()]) / (len(sentence.split()) + 0.001)\n",
    "    return np.zeros((100,))\n",
    "\n",
    "preprocess_ds(glove_vectorize)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Algorithm core"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def text_rank_summary(document, orig_document, n=1):\n",
    "    sim_mat = np.zeros([len(document), len(document)])\n",
    "    for i in range(len(document)):\n",
    "        for j in range(len(document)):\n",
    "            if i != j:\n",
    "                sim_mat[i][j] = cosine_similarity(document[i].reshape(1,100), document[j].reshape(1,100))[0,0]\n",
    "    nx_graph = nx.from_numpy_array(sim_mat)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "    # print(list(scores.items()))\n",
    "    # scores = scores.values()\n",
    "    sort_order = sorted(scores.keys(), reverse=True, key=lambda i: scores[i])\n",
    "    # print(sort_order)\n",
    "    return [orig_document[k] for k in sort_order[:n]]\n",
    "\n",
    "    # ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(document)), reverse=True)\n",
    "    # return ranked_sentences[:n]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The full cost of damage in Newton Stewart, one of the areas worst affected, is still being assessed.']\n",
      "['Insp David Gibson said: \"It appears as though the fire started under one of the buses before spreading to the second.']\n",
      "['Ferrari appeared in a position to challenge until the final laps, when the Mercedes stretched their legs to go half a second clear of the red cars.']\n"
     ]
    }
   ],
   "source": [
    "print(text_rank_summary(documents[0], orig_documents[0], 1))\n",
    "print(text_rank_summary(documents[1], orig_documents[1], 1))\n",
    "print(text_rank_summary(documents[2], orig_documents[2], 1))\n",
    "# print(orig_documents[:2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gensim - use TextRank from lib"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim.summarization'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[0;32mIn [29]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mgensim\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgensim\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msummarization\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpagerank_weighted\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pagerank_weighted\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'gensim.summarization'"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.summarization.pagerank_weighted import pagerank_weighted"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# let's count average size of summarization\n",
    "# ratios = []\n",
    "# word_counts ="
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print(summarize(orig_documents[0]))\n",
    "# print(summarize(orig_documents[1]))\n",
    "# print(summarize(orig_documents[2]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}